# **HASKELL FOR BEGINNERS   MODULE 3 , 4**

### **1. The Problem: How We Normally Write Math**

When you write math like this:

```
 1 + 2
(2 Ã— 3) + 5
3 Ã— (5 + 7)
(1 + 2) Ã— (3 + 4)
```

youâ€™re using **infix notation** the operator (`+`, `Ã—`) is *in between* the operands.

This feels natural to humans, but computers actually hate it.



### **2. Why Infix Notation Is Hard for Computers**

Infix notation looks simple to us because *we* remember rules like:

* â€œMultiply and divide before you add or subtract.â€
* â€œParentheses go first.â€

But computers donâ€™t have intuition   they have to **explicitly track precedence** and **grouping**.

So to evaluate something like:

```
3 Ã— (4 + 5)
```

a program must:

1. Parse the string character by character.
2. Detect parentheses.
3. Store operators and operands.
4. Apply precedence rules.

Thatâ€™s a lot of work   and it gets messier the deeper the nesting goes.

 

### **3. Alternative Ways to Write Math**

There are **three common notations** for arithmetic expressions:

| Notation                      | Example (for 1 + 2) | Operator position |
|          -- |       - |      -- |
| Infix                         | `1 + 2`             | between operands  |
| Prefix (Polish)               | `+ 1 2`             | before operands   |
| Postfix (Reverse Polish, RPN) | `1 2 +`             | after operands    |

 

### **4. The Key Idea of Postfix (RPN)**

In **postfix notation**, the operator comes *after* its operands.

For example:



| **Infix Expression** | **RPN (Postfix)** | **Reads As**                           |
| -------------------- | ----------------- | -------------------------------------- |
| `1 + 2`              | `1 2 +`           | Add 1 and 2                            |
| `(2 Ã— 3) + 5`        | `2 3 Ã— 5 +`       | Multiply 2 and 3, then add 5           |
| `3 Ã— (5 + 7)`        | `3 5 7 + Ã—`       | Add 5 and 7, then multiply by 3        |
| `(1 + 2) Ã— (3 + 4)`  | `1 2 + 3 4 + Ã—`   | Add 1 & 2, add 3 & 4, multiply results |


No parentheses.
No operator precedence.
Just a **clear, linear sequence** that can be evaluated easily.

 

### **5. Why RPN Is So Powerful**

RPN has a superpower: itâ€™s **unambiguous**.

Thereâ€™s only one way to read `1 2 + 3 Ã—`   it means â€œ(1 + 2) Ã— 3â€, not â€œ1 + (2 Ã— 3)â€.

No need for parentheses, and no rules like *â€œmultiplication before additionâ€*.
The order of tokens alone determines the order of evaluation.

 

### **6. How RPN Works   The Stack Mental Model**

To evaluate an RPN expression, you use a **stack**.

A **stack** is just a list where you can:

* **Push** an item onto the top.
* **Pop** an item from the top.

Think of a stack of plates   you always take or add from the top.

 

### **7. How the Stack Evaluates an Expression**

Letâ€™s take this RPN example:

```
1 2 + 3 4 + Ã—
```

Weâ€™ll evaluate it step by step.



| **Step** | **Token** | **Action**                     | **Stack (top on left)** |
| :------: | :-------: | :----------------------------- | :---------------------: |
|     1    |    `1`    | push 1                         |          `[1]`          |
|     2    |    `2`    | push 2                         |         `[2, 1]`        |
|     3    |    `+`    | pop 2 & 1 â†’ add â†’ push 3       |          `[3]`          |
|     4    |    `3`    | push 3                         |         `[3, 3]`        |
|     5    |    `4`    | push 4                         |       `[4, 3, 3]`       |
|     6    |    `+`    | pop 4 & 3 â†’ add â†’ push 7       |         `[7, 3]`        |
|     7    |    `Ã—`    | pop 7 & 3 â†’ multiply â†’ push 21 |          `[21]`         |


End of expression, stack has one value â†’ **21** âœ…


1. **Tokenizer (lexer)**   splits the input string into tokens (numbers, operators).

   * Weâ€™ll use `words` as tokenizer: `"4 10 - 20 + *"` â†’ `["4","10","-","20","+","*"]`.

2. **Parser**   in RPN the parser is trivial-ish: classify tokens as number or operator and (optionally) build a small token AST. For infix languages the parser is heavier; for RPN tokens themselves almost *are* the parsed representation.

3. **Type-checker** (optional)   ensure operators match operand counts; ensure types are correct. For our numbers-only calculator this is simple.

4. **Run-time environment**   here that is the stack: push/pop operations and arithmetic primitives.

So building an RPN evaluator is literally building a mini compiler pipeline: lex â†’ parse â†’ evaluate.

 

## 7 Safe Haskell implementation   step-by-step, explained

Below is a short, robust, beginner-friendly implementation. Read the code, then Iâ€™ll explain each piece.

```haskell
-- RPN evaluator: safe, handles parsing errors

import Text.Read (readMaybe)
import Control.Monad (foldM)

-- Evaluate an RPN expression given as a string.
-- Returns either an error message (Left) or the computed Double (Right).
runRPN :: String -> Either String Double
runRPN input = do
  let tokens = words input
  finalStack <- foldM step [] tokens        -- foldM threads Either through steps
  case finalStack of
    [result] -> Right result
    []       -> Left "empty expression"
    _        -> Left $ "leftover values on stack: " ++ show finalStack

-- A single step consumes one token and updates the stack (List Double)
-- step :: [Double] -> String -> Either String [Double]
step :: [Double] -> String -> Either String [Double]
step stack tok =
  case tok of
    "+" -> binaryOp stack (+)
    "-" -> binaryOp stack (-)
    "*" -> binaryOp stack (*)
    "/" -> safeDiv stack
    "^" -> binaryOp stack (**)
    _   -> case readMaybe tok :: Maybe Double of
             Just n  -> Right (n : stack)         -- push number
             Nothing -> Left $ "unknown token: " ++ show tok

-- binaryOp pops two numbers x (top) and y (next), computes y `op` x and pushes result
binaryOp :: [Double] -> (Double -> Double -> Double) -> Either String [Double]
binaryOp (x:y:rest) op = Right (op y x : rest)
binaryOp _ _           = Left "stack underflow: not enough operands for binary operator"

-- handle division with divide-by-zero check
safeDiv :: [Double] -> Either String [Double]
safeDiv (x:y:rest)
  | x == 0    = Left "division by zero"
  | otherwise = Right (y / x : rest)
safeDiv _ = Left "stack underflow: not enough operands for /"
```

### **Built-in Example: Maybe**

```haskell
data Maybe a = Nothing | Just a
```

Used for computations that *might fail*.

```haskell
safeDiv :: Int -> Int -> Maybe Int
safeDiv _ 0 = Nothing
safeDiv x y = Just (x `div` y)
```

Now the compiler *forces* you to handle both cases   no â€œnull pointerâ€ drama.

 

### **Another Example: Either**

```haskell
data Either a b = Left a | Right b
```

`Either` is like `Maybe`, but carries *two possible outcomes*   success or error with info.

```haskell
login :: String -> String -> Either String String
login username password
  | password == "admin123" = Right "Access granted"
  | otherwise               = Left "Wrong password"
```

This pattern is central to Haskellâ€™s **error handling** philosophy   explicit, not silent.


### **Analogy**

Think of data types like *building blueprints*.
Constructors are the â€œroomsâ€ inside that blueprint.
When you create a value, youâ€™re walking into one specific room  Haskell knows exactly where you are.



## **2. RPN CALCULATOR (Reverse Polish Notation)**

If youâ€™ve ever seen a calculator that doesnâ€™t use parentheses, itâ€™s probably using **Reverse Polish Notation (RPN)**.

RPN writes operators **after** their operands:

```
3 4 +   â†’  7
10 2 /  â†’  5
```

 

### **How It Works**

It uses a **stack**:

* Push numbers onto the stack.
* When you see an operator, pop the top two numbers, apply the operation, push the result back.

 

### **Haskell Implementation**

```haskell
import Data.List

solveRPN :: String -> Float
solveRPN = head . foldl foldingFunction [] . words
  where
    foldingFunction (x:y:ys) "+" = (y + x):ys
    foldingFunction (x:y:ys) "-" = (y - x):ys
    foldingFunction (x:y:ys) "*" = (y * x):ys
    foldingFunction xs number    = read number : xs
```

Try:

```haskell
solveRPN "10 4 3 + 2 * -"   -- 10 - ((4 + 3) * 2) = -4
```

 

### **Conceptual Breakdown**

* `words` splits the string into tokens.
* `foldl` walks through them, maintaining a stack (`[Float]`).
* When we see a number, we push it.
* When we see an operator, we pop two and apply it.
* `head` grabs the final result.

 

**Why this matters:**
Itâ€™s a small, self-contained **interpreter**.
Youâ€™ve just written a tiny *compiler pipeline* from parsing text to computing a result.

 

## **3. COMPILER STRUCTURE (Simplified)**

## What Even *Is* a Compiler?

Think of a compiler like a **translator**.
It takes what *you* write (human-readable code) and turns it into what the **machine** understands (binary instructions).

But hereâ€™s the cool part , this translation doesnâ€™t happen in one go. It goes through a series of well-defined stages, like checkpoints in a delivery pipeline.


##  Stage 1: Lexing (Tokenizing)

**Goal:** Break your code into small, meaningful pieces   *tokens.*

Example:
You type this:

```haskell
3 + 4
```

Lexing breaks it into:

```haskell
["3", "+", "4"]
```

Each piece (like `3`, `+`, `4`) is a **token**   a tiny symbol with meaning.

**Analogy:**
Imagine reading a recipe:

> "Add 2 cups of flour."

Lexing is like splitting that into:

```
["Add", "2", "cups", "of", "flour"]
```

Now, the computer can at least see the individual words.

**In your RPN calculator:**

```haskell
words "3 4 +"
-- ["3", "4", "+"]
```

Congrats   thatâ€™s lexing! 


##  Stage 2: Parsing

**Goal:** Figure out *structure* - how the tokens relate to each other.
Lexing gives raw pieces, but parsing builds a **tree** that represents how they connect.

Example:

```
"3 + 4 * 2"
```

After parsing, we get something like:

```
      (+)
     /   \
   (3)   (*)
        /   \
      (4)   (2)
```

This is called an **abstract syntax tree (AST)**   itâ€™s how compilers â€œseeâ€ your program.


 **Analogy:**
Think of tokens like LEGO bricks.
Parsing is how you snap them together to build the castle.

ðŸ’¡ **In Haskell terms:**
You might define:

```haskell
data Expr = Val Double
          | Add Expr Expr
          | Mul Expr Expr
```

Then parsing turns text like `"3 + 4 * 2"` into structured data like:

```haskell
Add (Val 3) (Mul (Val 4) (Val 2))
```

Now Haskell can *reason* about your code.


##  Stage 3: Evaluation

**Goal:** Actually *do the thing.*

Now that we have a structured expression, we just recursively evaluate it.

```haskell
eval (Val x)     = x
eval (Add a b)   = eval a + eval b
eval (Mul a b)   = eval a * eval b
```

So:

```haskell
eval (Add (Val 3) (Mul (Val 4) (Val 2)))  -- 3 + (4 * 2)
-- Result: 11
```

**Analogy:**
Now that the LEGO castle is built, this is when you *play with it.*

**In your RPN calculator:**
You already did this with:

```haskell
foldM step [] (words expr)
```

Thatâ€™s evaluation. Each step combines numbers and operators.


##  Real-World Scenarios

1. **Interpreters & DSLs (Domain-Specific Languages)**
   You could make a mini language inside Haskell   like a config DSL, math expression evaluator, or even a chatbot scripting language.
   (Thatâ€™s basically what your RPN calculator is doing.)

2. **Template Haskell**
   Haskell can manipulate *its own code* at compile time. Itâ€™s meta-programming, like writing a compiler inside your compiler.
   Example: generating boilerplate functions automatically.

3. **Static Analysis Tools**
   You can build tools that read Haskell code, parse it, and check for style or logic errors   same logic as a compiler front-end.

4. **Game Logic or Expression Evaluators**
   Ever seen games where users type formulas or AI scripts?
   Youâ€™re basically building a mini-compiler for those inputs.


##  Analogy Recap

| Compiler Stage | What It Does           | Analogy                      | Your RPN Example             |
| -------------- | ---------------------- | ---------------------------- | ---------------------------- |
| **Lexing**     | Break text into tokens | Splitting recipe words       | `words`                      |
| **Parsing**    | Build structure (AST)  | Assembling LEGO bricks       | (Not in RPN â€“ itâ€™s implicit) |
| **Evaluation** | Compute meaning        | Playing with the LEGO castle | `foldl` / `foldM`            |







##  Whatâ€™s an Accumulator, Really?

An **accumulator** is just a variable that *remembers progress* while your recursive function runs.

Think of recursion as a loop that doesnâ€™t mutate state   instead of updating a variable like in Python or C, you *pass* the new state along in the next recursive call.

So the **accumulator carries your work forward** each step of the way.


##  Start Simple: The Regular Recursive Sum

Hereâ€™s the naive version:

```haskell
sumList [] = 0
sumList (x:xs) = x + sumList xs
```

Letâ€™s trace:

```
sumList [1,2,3,4]
= 1 + sumList [2,3,4]
= 1 + (2 + sumList [3,4])
= 1 + (2 + (3 + sumList [4]))
= 1 + (2 + (3 + (4 + sumList [])))
= 1 + (2 + (3 + (4 + 0)))
```

See the buildup?
Haskell has to **hold onto each â€œ+â€** until it reaches the base case.
That means a stack of *pending operations*:

```
(+ 1)
(+ 2)
(+ 3)
(+ 4)
```

This can blow the stack with large lists   like walking down 10,000 stairs and leaving a note at every step saying, *â€œDonâ€™t forget to add me later!â€*


##  Fix It: Add an Accumulator

Hereâ€™s the improved version:

```haskell
sumAcc :: Int -> [Int] -> Int
sumAcc acc [] = acc
sumAcc acc (x:xs) = sumAcc (acc + x) xs
```

Now, start it like:

```haskell
sumAcc 0 [1,2,3,4]
```

Letâ€™s trace:

```
sumAcc 0 [1,2,3,4]
â†’ sumAcc (0 + 1) [2,3,4]
â†’ sumAcc (1 + 2) [3,4]
â†’ sumAcc (3 + 3) [4]
â†’ sumAcc (6 + 4) []
â†’ 10
```

Each step *does the math immediately*, no waiting.
No growing chain, no pending adds. Just clean, **tail recursion**   memory efficient and fast.

 Haskell can even **optimize this** into a loop under the hood.


##  Real-Life Analogy

Imagine youâ€™re counting coins while walking down stairs.

* **Without accumulator:**
  You drop each coin at every step and tell yourself, â€œIâ€™ll add these later.â€
  When you reach the bottom, youâ€™ve got to climb back up collecting them. 

* **With accumulator:**
  You just keep a running total in your hand.
  By the time you hit the last step   youâ€™re done. 

Thatâ€™s an accumulator. Itâ€™s the â€œIâ€™ll keep track of this as I goâ€ trick.

##  The â€œFor Loopâ€ Parallel

In imperative languages, youâ€™d do:

```python
total = 0
for x in [1,2,3,4]:
    total += x
```

`total` here **is** the accumulator   same logic, just passed along recursively in Haskell instead of mutated.


##  Step-by-Step: Building One Yourself

Letâ€™s take a classic   reverse a list.

###  Without an accumulator:

```haskell
reverseList [] = []
reverseList (x:xs) = reverseList xs ++ [x]
```

Trace it for `[1,2,3]`:

```
reverseList [1,2,3]
= reverseList [2,3] ++ [1]
= (reverseList [3] ++ [2]) ++ [1]
= ((reverseList [] ++ [3]) ++ [2]) ++ [1]
= ([] ++ [3] ++ [2] ++ [1])
= [3,2,1]
```

But thatâ€™s **slow**, because `(++)` rebuilds the list every time.


###  With an accumulator:

```haskell
reverseAcc :: [a] -> [a] -> [a]
reverseAcc acc [] = acc
reverseAcc acc (x:xs) = reverseAcc (x:acc) xs
```

Start it like:

```haskell
reverseAcc [] [1,2,3]
```

Trace:

```
reverseAcc [] [1,2,3]
â†’ reverseAcc [1] [2,3]
â†’ reverseAcc [2,1] [3]
â†’ reverseAcc [3,2,1] []
â†’ [3,2,1]
```

Boom   no list rebuilding, no waiting. Youâ€™re stacking as you go.


##  Why It Matters in Haskell

* **Efficiency:** avoids stack overflow on large inputs
* **Tail recursion:** can be compiled as a loop
* **Purity:** no mutable state, just new values passed along
* **Control:** you can track partial progress (like counters, sums, logs, etc.)

##  Real-World Use Cases

1. **Counting Things Efficiently**

   ```haskell
   countEven acc [] = acc
   countEven acc (x:xs)
     | even x    = countEven (acc + 1) xs
     | otherwise = countEven acc xs
   ```

2. **Building Strings**

   ```haskell
   concatAcc acc [] = acc
   concatAcc acc (x:xs) = concatAcc (acc ++ x) xs
   ```

3. **Simulating Loops**
   You can simulate progress counters, sums, or running averages using accumulators   like a `foldl` under the hood.


##  Think of It Like This

| Style                   | How It Thinks                      | Memory    | Example        |
| ----------------------- | ---------------------------------- | --------- | -------------- |
| Regular recursion       | â€œIâ€™ll finish later.â€               | High      | `sumList`      |
| Accumulator recursion   | â€œLetâ€™s do it now.â€                 | Low       | `sumAcc`       |
| Fold (`foldl`, `foldr`) | â€œGeneralized accumulator pattern.â€ | Optimized | Built-in loops |


##  Mental Shortcut

Whenever you write recursion and notice:

> â€œHmm, Iâ€™m building up a big expression or list before finishing...â€

Thatâ€™s your cue to introduce an **accumulator**.

You can literally transform:

```haskell
f [] = base
f (x:xs) = combine x (f xs)
```

into

```haskell
fAcc acc [] = acc
fAcc acc (x:xs) = fAcc (combine' acc x) xs
```

Boom   same logic, just done *as you go*.


## **5. Tail Recursion  : The â€œSmartâ€ Recursion

In normal recursion, every function call waits for the *next* one to finish before it can complete.
Tail recursion flips that  : it does all the work *now*, and then jumps straight to the next call with **nothing left to remember**.

So instead of building up a mountain of â€œto-dos,â€ tail recursion just walks through the steps one by one  : constant memory, no clutter.


##  Example: The Tail-Recursive Factorial

Letâ€™s take the standard version first:

```haskell
fact :: Integer -> Integer
fact 0 = 1
fact n = n * fact (n - 1)
```

Letâ€™s trace `fact 5`:

```
fact 5
= 5 * fact 4
= 5 * (4 * fact 3)
= 5 * (4 * (3 * fact 2))
= 5 * (4 * (3 * (2 * fact 1)))
= 5 * (4 * (3 * (2 * (1 * fact 0))))
= 5 * (4 * (3 * (2 * (1 * 1))))
= 120
```

 You can see how it **builds a chain of multiplications** that stays on the stack until the end.
If you call `fact 100000`, your stackâ€™s gonna explode.


##  Now, the Tail-Recursive Fix

```haskell
fact :: Integer -> Integer
fact n = helper n 1
  where
    helper 0 acc = acc
    helper n acc = helper (n - 1) (n * acc)
```

Letâ€™s trace `fact 5`:

```
helper 5 1
â†’ helper 4 (5 * 1)
â†’ helper 3 (4 * 5)
â†’ helper 2 (3 * 20)
â†’ helper 1 (2 * 60)
â†’ helper 0 (1 * 120)
â†’ 120
```

Every step *finishes immediately* and just passes the new result forward.

Thereâ€™s no â€œwaiting to multiply later.â€
Each call says, â€œI did my part  : you handle the next one.â€

 **No buildup, no recursion stack overflow, constant memory.**


##  How It Works Under the Hood

When a recursive call is the **last thing a function does**, Haskell (and other compilers) can **optimize** it into a simple loop.
Thatâ€™s what â€œtail recursionâ€ means  : the recursive call happens at the *tail* (the very end).

This optimization is called **Tail Call Optimization (TCO)**.
In essence, instead of adding another frame to the call stack, the compiler just *reuses* the current one.

So your function effectively becomes this:

```haskell
loop(n, acc):
  while n != 0:
    acc = n * acc
    n = n - 1
  return acc
```

Same logic, just in a loop disguise.


##  Tail Recursion vs Normal Recursion

| **Type**             | **Expression Chain**      | **Stack Usage**        | **Performance**  |
| -------------------- | ------------------------- | ---------------------- | ---------------- |
| **Normal Recursion** | `n * (n-1 * (n-2 * ...))` | Builds up stack (O(n)) | Slower for big n |
| **Tail Recursion**   | `helper n acc`            | Constant memory (O(1)) | Super efficient  |


##  Analogy Time

Imagine climbing a staircase:

* **Normal recursion:**
  You climb up 5 floors, but at every floor, you drop a sticky note saying â€œMultiply by this later.â€
  When you reach the top, you have to go *all the way back down* reading the notes and multiplying. 

* **Tail recursion:**
  You carry your result in your pocket  : multiply as you go up.
  By the time you reach the top, youâ€™re done. No return trip. 

Thatâ€™s tail recursion  : **do it now, move on.**


##  Real-World Uses

* **Iterative computations**  : factorials, sums, counters, Fibonacci (with accumulator)
* **Traversing large lists safely**  : prevents stack overflow
* **Simulating loops**  : since Haskell doesnâ€™t have mutable loops, tail recursion *is* your loop
* **Efficient algorithms**  : parsing, searching, traversing trees, etc.


##  Key Insight

Every tail-recursive function has **an accumulator hiding somewhere**.
That accumulator carries progress, turning a recursive pattern into a loop.

So the â€œrecipeâ€ for writing one is:

1. Add an accumulator parameter (to store current progress).
2. Make the recursive call the *last* thing you do.
3. Pass the updated accumulator forward.

Example pattern:

```haskell
loop acc []     = acc
loop acc (x:xs) = loop (acc + x) xs
```

Thatâ€™s tail recursion. Thatâ€™s how you get efficiency *without losing purity.*


##  Bonus: Fibonacci Comparison

Normal version (beautiful but slow):

```haskell
fib 0 = 0
fib 1 = 1
fib n = fib (n-1) + fib (n-2)
```

Tail-recursive version:

```haskell
fibTR :: Integer -> Integer
fibTR n = helper n 0 1
  where
    helper 0 a _ = a
    helper n a b = helper (n - 1) b (a + b)
```

Now it runs like a loop, doesnâ€™t blow the stack, and finishes fast.


##  TL;DR

* **Accumulator** = keeps track of progress.
* **Tail recursion** = recursion with no backlog; acts like a loop.
* **Both together** = efficient, pure, and scalable functional code.

> Youâ€™re not â€œavoiding loopsâ€ in Haskell  : youâ€™re *writing them smarter.*



## **6. COMPUTATIONAL COMPLEXITY**


###  What Computational Complexity Actually Means

When we talk about *complexity*, weâ€™re talking about how the **time** and **memory** used by your program grow as the input gets bigger.

Itâ€™s the difference between:

* A function that scales *smoothly* with input size
* vs. one that explodes faster than your patience.

In short:

> **How much work does your code do?**

And yes  , Haskell may look mathy and chill, but itâ€™s not magic.
Those clean one-liners still boil down to loops, stacks, and memory under the hood.


##  List Operations: The Baseline

Most Haskell data structures are *linked lists*  , so traversing them is **O(n)** because you have to touch each element once.

| Function          | What it does                        | Time Complexity | Notes    |
| ----------------- | ----------------------------------- | --------------- | -------- |
| `map`             | applies a function to every element | O(n)            | one pass |
| `filter`          | keeps only matching elements        | O(n)            | one pass |
| `foldr` / `foldl` | reduces a list into a single value  | O(n)            | one pass |
| `length`          | counts elements                     | O(n)            | one pass |

Every one of these goes element by element. Thereâ€™s no skipping  , Haskell laziness might delay *when* it happens, but not *how much* work there is.


##  Example 1  , Linear Complexity (`O(n)`)

```haskell
sum (map (^2) [1..n])
```

Breakdown:

1. `[1..n]` â†’ generates `n` elements
2. `map (^2)` â†’ applies `(^2)` to each â†’ O(n)
3. `sum` â†’ adds them all â†’ O(n)

So total work = **O(n)** + **O(n)** = still **O(n)**.
It scales directly with input size.

 If `n` doubles, runtime roughly doubles too. Clean.



##  Example 2  , Quadratic Complexity (`O(nÂ²)`)

```haskell
[ [x*y | y <- [1..n]] | x <- [1..n] ]
```

Hereâ€™s what happens:

* The outer list comprehension runs **n** times (for each `x`)
* The inner one runs **n** times per `x`

Thatâ€™s **n Ã— n = nÂ²** total operations.

 So if `n = 1000`, youâ€™re doing **1,000,000 multiplications**.
Big jump  , and thatâ€™s how `O(nÂ²)` feels in real life.



##  Haskellâ€™s Lazy Twist

Haskell is **lazy**  , it doesnâ€™t evaluate stuff until itâ€™s needed.

That can be your superpowerâ€¦ or your trap.

###  Laziness helps when:

You only need part of the data.

```haskell
take 5 (map (^2) [1..])
-- only computes 1Â², 2Â², 3Â², 4Â², 5Â²
```

Even though `[1..]` is infinite, Haskell computes just enough  , O(5).
Thatâ€™s the magic of *on-demand evaluation.*

###  Laziness hurts when:

You *build up* too many unevaluated expressions (called **thunks**).

Example:

```haskell
foldl (+) 0 [1..1000000]
```

This doesnâ€™t actually perform addition immediately.
It creates a massive chain like:

```
(((0 + 1) + 2) + 3) + 4 + ...
```

Thatâ€™s **O(n)** in time *and* **O(n)** in memory**, because it stores that whole unevaluated chain.

##  Key Takeaway

Even though Haskell looks declarative and pure:

* **Time complexity still matters**  , `map`, `filter`, `fold` are O(n).
* **Nested loops or recursion** blow up fast  , O(nÂ²), O(nÂ³), etc.
* **Laziness** can hide memory leaks if youâ€™re not careful  , watch your thunks.


##  Real-Life Analogy

Imagine youâ€™re cleaning your room.

* `map` = walk around once, polish everything (O(n))
* `filter` = walk around once, throw out junk (O(n))
* `nested map` = for every item, clean it *and* all its parts again (O(nÂ²))
* `lazy evaluation` = you *say* youâ€™ll clean everything later, but pile up a million â€œto-dosâ€ in your head (memory explosion )

So yeah, even in the functional world  , **efficiency still hits hard.**


##  Quick Cheat Table

| Operation           | Time  | Memory             | Lazy?                | Notes                |
| ------------------- | ----- | ------------------ | -------------------- | -------------------- |
| `map`               | O(n)  | O(n)               | âœ…                    | delayed until needed |
| `filter`            | O(n)  | O(n)               | âœ…                    | builds new list      |
| `foldr`             | O(n)  | O(n)               | âœ…                    | works right-to-left  |
| `foldl`             | O(n)  |  can blow memory   | âœ…                    | lazy accumulator     |
| `concat (map f xs)` | O(nÂ²) | O(nÂ²)              | âœ…                    | watch out            |


##  TL;DR

* Haskellâ€™s elegance doesnâ€™t erase computational cost.
* Laziness saves you from doing unnecessary work, **not** from expensive code.
* Write declaratively, but **think operationally.**
* When in doubt  , reason like itâ€™s a loop.

> Haskell lets you *write math*  , but your CPU still runs instructions. 



## **7. GLOBAL VS LOCAL NAMES**

###  **Global name**

Declared at the top level â€” visible everywhere in the module.

```haskell
square x = x * x

area r = pi * square r
```

Here, `square` is *global*, so **any other function** in this file (or imported module) can call it.

Thatâ€™s fine for utilities, but messy if itâ€™s just a helper used once.


###  **Local name**

Defined *inside* another function using a `where` or `let`.
Only that function can see it.

```haskell
area r = pi * square r
  where square x = x * x
```

Now `square` is *local to* `area`.
Outside `area`, it doesnâ€™t exist. Clean and private.


###  **Same idea with `let`**

```haskell
area r =
  let square x = x * x
  in  pi * square r
```

`where` puts the helper *after* the main expression,
`let...in` puts it *before*.
Otherwise, same concept.


###  Why you should care

* **Encapsulation** â€“ hides irrelevant stuff from the rest of the file.
* **Name safety** â€“ avoids clashing with other functions named `square`.
* **Readability** â€“ makes it clear which helpers belong to which function.


###  TL;DR

| Scope  | Declared in     | Visible to   | Example                  |
| ------ | --------------- | ------------ | ------------------------ |
| Global | Top level       | Whole module | `square x = x * x`       |
| Local  | `where` / `let` | One function | `where square x = x * x` |

> Think: global = â€œpublic APIâ€, local = â€œprivate helper.â€


## **8. FILTER**

Weâ€™ve seen it before, but now we treat it as a *pattern*.

```haskell
filter :: (a -> Bool) -> [a] -> [a]
filter _ [] = []
filter p (x:xs)
  | p x       = x : filter p xs
  | otherwise = filter p xs
```

 

### **Usage**

```haskell
filter even [1,2,3,4,5]  -- [2,4]
filter (>3) [1..6]       -- [4,5,6]
```

 

**Analogy:**
`filter` is like a nightclub bouncer.
Each element walks up, the predicate decides if it gets in.

 

## **9. ZIP & ZIPWITH**

### **Zip**

```haskell
zip [1,2,3] ["a","b","c"]
-- [(1,"a"), (2,"b"), (3,"c")]
```

It pairs elements positionally until one list runs out.

 

### **ZipWith**

More general lets you define how to combine.

```haskell
zipWith (+) [1,2,3] [4,5,6]
-- [5,7,9]
```

You can zip in creative ways:

```haskell
zipWith (++) ["Hi","Ha"] [" there","skell"]
-- ["Hi there", "Haskell"]
```

**Analogy:**
Think of two parallel conveyor belts.
`zip` just couples boxes; `zipWith` fuses them however you want.

 

